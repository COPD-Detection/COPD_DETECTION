{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9802014,"sourceType":"datasetVersion","datasetId":6007592},{"sourceId":9846132,"sourceType":"datasetVersion","datasetId":6041081},{"sourceId":10512806,"sourceType":"datasetVersion","datasetId":6507349},{"sourceId":10516576,"sourceType":"datasetVersion","datasetId":6509552}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"print(\"Hello\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T13:56:04.361727Z","iopub.execute_input":"2025-01-19T13:56:04.362003Z","iopub.status.idle":"2025-01-19T13:56:04.366645Z","shell.execute_reply.started":"2025-01-19T13:56:04.361977Z","shell.execute_reply":"2025-01-19T13:56:04.365866Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nprint(os.listdir(\"/kaggle/working\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T04:53:14.543164Z","iopub.execute_input":"2025-01-16T04:53:14.543509Z","iopub.status.idle":"2025-01-16T04:53:14.548467Z","shell.execute_reply.started":"2025-01-16T04:53:14.54348Z","shell.execute_reply":"2025-01-16T04:53:14.547608Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\n# Define the directory to delete\ndirectory_to_delete = \"/kaggle/working/best_model_mel.pth\"\n\n# Delete the directory and its contents\ntry:\n    shutil.rmtree(directory_to_delete)\n    print(f\"Successfully deleted: {directory_to_delete}\")\nexcept FileNotFoundError:\n    print(f\"Directory not found: {directory_to_delete}\")\nexcept Exception as e:\n    print(f\"Error deleting directory: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T15:28:07.605866Z","iopub.execute_input":"2025-01-19T15:28:07.606643Z","iopub.status.idle":"2025-01-19T15:28:07.611983Z","shell.execute_reply.started":"2025-01-19T15:28:07.60661Z","shell.execute_reply":"2025-01-19T15:28:07.61106Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# Path to the working directory\ndirectory_to_delete = \"/kaggle/working\"\n\n# Iterate over items in the directory\nfor item in os.listdir(directory_to_delete):\n    item_path = os.path.join(directory_to_delete, item)\n    \n    # Check if the item is a file\n    if os.path.isfile(item_path):\n        os.remove(item_path)  # Delete the file\n        print(f\"Deleted file: {item_path}\")\n    else:\n        print(f\"Skipped folder: {item_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T13:48:18.691035Z","iopub.execute_input":"2025-01-15T13:48:18.691727Z","iopub.status.idle":"2025-01-15T13:48:18.696431Z","shell.execute_reply.started":"2025-01-15T13:48:18.691695Z","shell.execute_reply":"2025-01-15T13:48:18.695659Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport shutil\n\n# Source directory (COPD Levels folder)\nsource_directory = \"/kaggle/input/split-dataset-2/Split_Dataset2\"\n\n# Destination directory\ndestination_directory = \"/kaggle/working/Split_Dataset2\"\n\n# Ensure the destination directory exists\nos.makedirs(destination_directory, exist_ok=True)\n\n# Walk through all folders in the source directory\nfor root, dirs, files in os.walk(source_directory):\n    # Create a corresponding folder structure in the destination\n    relative_path = os.path.relpath(root, source_directory)\n    target_path = os.path.join(destination_directory, relative_path)\n    os.makedirs(target_path, exist_ok=True)\n    \n    # Copy all files in the current directory\n    for file_name in files:\n        source_path = os.path.join(root, file_name)\n        destination_path = os.path.join(target_path, file_name)\n        shutil.copy(source_path, destination_path)\n        print(f\"Copied: {source_path} to {destination_path}\")\n\nprint(f\"All files and folders have been successfully copied to {destination_directory}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T15:26:53.34322Z","iopub.execute_input":"2025-01-19T15:26:53.343586Z","iopub.status.idle":"2025-01-19T15:27:08.895614Z","shell.execute_reply.started":"2025-01-19T15:26:53.343547Z","shell.execute_reply":"2025-01-19T15:27:08.894715Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom pydub import AudioSegment\n\n# Input and output directories\ninput_directory = \"/kaggle/working/COPD_Levels\"\noutput_directory = \"/kaggle/working/Preprocessed_COPD_Levels\"\n\n# Ensure output directory exists\nos.makedirs(output_directory, exist_ok=True)\n\n# Function to segment audio files\ndef segment_audio(file_path, output_path, segment_duration=10*1000, overlap_duration=2*1000):\n    \"\"\"\n    Segments an audio file into fixed intervals with overlapping.\n    \n    Args:\n        file_path (str): Path to the input audio file.\n        output_path (str): Path to save the segmented files.\n        segment_duration (int): Duration of each segment in milliseconds (default 10 seconds).\n        overlap_duration (int): Overlap duration between segments in milliseconds (default 2 seconds).\n    \"\"\"\n    # Load the audio file\n    audio = AudioSegment.from_file(file_path)\n    audio_length = len(audio)\n    \n    # Calculate start and end times for each segment\n    start = 0\n    while start + segment_duration <= audio_length:\n        end = start + segment_duration\n        segment = audio[start:end]\n        \n        # Save the segment\n        segment_name = f\"{os.path.splitext(os.path.basename(file_path))[0]}_{start // 1000}_{end // 1000}.wav\"\n        segment.export(os.path.join(output_path, segment_name), format=\"wav\")\n        \n        start += segment_duration - overlap_duration  # Move forward with overlap\n\n    # Handle the remaining part (if any) by creating a final segment\n    if start < audio_length:\n        segment = audio[-segment_duration:]  # Take the last segment of 10 seconds\n        segment_name = f\"{os.path.splitext(os.path.basename(file_path))[0]}_final.wav\"\n        segment.export(os.path.join(output_path, segment_name), format=\"wav\")\n\n# Process all COPD level folders and audio files\nfor root, dirs, files in os.walk(input_directory):\n    for file_name in files:\n        if file_name.endswith(\".wav\"):  # Adjust the file extension if needed\n            input_file_path = os.path.join(root, file_name)\n            \n            # Determine the corresponding output folder\n            relative_path = os.path.relpath(root, input_directory)\n            output_folder = os.path.join(output_directory, relative_path)\n            os.makedirs(output_folder, exist_ok=True)\n            \n            # Segment the audio file\n            segment_audio(input_file_path, output_folder)\n\nprint(f\"Preprocessing complete! Segmented audio files are saved in: {output_directory}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T13:56:20.147431Z","iopub.execute_input":"2025-01-15T13:56:20.148184Z","iopub.status.idle":"2025-01-15T13:56:20.496384Z","shell.execute_reply.started":"2025-01-15T13:56:20.148152Z","shell.execute_reply":"2025-01-15T13:56:20.495504Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install soundfile\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T14:14:40.97962Z","iopub.execute_input":"2025-01-15T14:14:40.979967Z","iopub.status.idle":"2025-01-15T14:14:50.056407Z","shell.execute_reply.started":"2025-01-15T14:14:40.979937Z","shell.execute_reply":"2025-01-15T14:14:50.055525Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport librosa\nimport numpy as np\nimport soundfile as sf  # Import soundfile\n\ndef apply_pitch_scaling(audio_file, output_path, sr=22050, n_steps=2):\n    # Load the audio file\n    y, sr = librosa.load(audio_file, sr=sr)\n    # Apply pitch scaling (shift by `n_steps` semitones)\n    y_shifted = librosa.effects.pitch_shift(y, sr=sr, n_steps=n_steps)\n    # Save the augmented audio using soundfile\n    file_name = os.path.basename(audio_file).replace(\".wav\", f\"_pitch_{n_steps}.wav\")\n    sf.write(os.path.join(output_path, file_name), y_shifted, sr)  # Use soundfile.write\n\ndef apply_noise_addition(audio_file, output_path, noise_level=0.005, sr=22050):\n    # Load the audio file\n    y, sr = librosa.load(audio_file, sr=sr)\n    # Generate white noise\n    noise = np.random.normal(0, noise_level, y.shape)\n    # Add noise to the audio signal\n    y_noisy = y + noise\n    # Save the augmented audio using soundfile\n    file_name = os.path.basename(audio_file).replace(\".wav\", f\"_noise_{noise_level}.wav\")\n    sf.write(os.path.join(output_path, file_name), y_noisy, sr)  # Use soundfile.write\n\ninput_dir = \"/kaggle/working/Preprocessed_COPD_Levels\"\noutput_dir = \"/kaggle/working/Augmented_COPD_Levels\"\n\n# Create output directories\nfor level in [\"COPD0\", \"COPD1\", \"COPD2\", \"COPD3\", \"COPD4\"]:\n    os.makedirs(os.path.join(output_dir, level), exist_ok=True)\n\n# Apply augmentations\nfor level in os.listdir(input_dir):\n    input_path = os.path.join(input_dir, level)\n    output_path = os.path.join(output_dir, level)\n    \n    for file in os.listdir(input_path):\n        audio_file = os.path.join(input_path, file)\n        \n        # Apply pitch scaling (e.g., shift by +2 and -2 semitones)\n        apply_pitch_scaling(audio_file, output_path, n_steps=2)\n        apply_pitch_scaling(audio_file, output_path, n_steps=-2)\n        \n        # Apply noise addition (e.g., noise level = 0.005)\n        apply_noise_addition(audio_file, output_path, noise_level=0.005)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T14:15:34.571948Z","iopub.execute_input":"2025-01-15T14:15:34.572768Z","iopub.status.idle":"2025-01-15T14:19:57.75643Z","shell.execute_reply.started":"2025-01-15T14:15:34.572728Z","shell.execute_reply":"2025-01-15T14:19:57.755595Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! pip install librosa","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T17:11:08.391916Z","iopub.execute_input":"2025-01-15T17:11:08.392268Z","iopub.status.idle":"2025-01-15T17:11:19.146027Z","shell.execute_reply.started":"2025-01-15T17:11:08.39224Z","shell.execute_reply":"2025-01-15T17:11:19.144804Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport librosa\nimport numpy as np\nimport torch\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport cv2\n\nclass AudioFeatureExtractor:\n    def __init__(self, sr=22050, device='cuda' if torch.cuda.is_available() else 'cpu'):\n        self.sr = sr\n        self.device = device\n        # Parameters for feature extraction\n        self.n_fft = 2048\n        self.hop_length = 512\n        self.n_mels = 128\n        self.n_chroma = 12\n        \n    def load_audio(self, audio_path):\n        \"\"\"Load audio file and convert to torch tensor\"\"\"\n        y, _ = librosa.load(audio_path, sr=self.sr)\n        return torch.FloatTensor(y).to(self.device)\n    \n    def save_feature_image(self, feature, output_path, vmin=None, vmax=None):\n        \"\"\"Save feature matrix as an image\"\"\"\n        plt.figure(figsize=(10, 4))\n        plt.imshow(feature, aspect='auto', origin='lower', cmap='viridis', vmin=vmin, vmax=vmax)\n        plt.axis('off')\n        plt.tight_layout()\n        plt.savefig(output_path, bbox_inches='tight', pad_inches=0)\n        plt.close()\n        \n        # Convert to grayscale and resize for consistency\n        img = cv2.imread(output_path, cv2.IMREAD_GRAYSCALE)\n        img = cv2.resize(img, (224, 224))  # Standard size for many CNN architectures\n        cv2.imwrite(output_path, img)\n    \n    def extract_features(self, audio_path, output_dir):\n        \"\"\"Extract and save all features for a single audio file\"\"\"\n        # Create output directories if they don't exist\n        feature_types = ['spectrogram', 'melspectrogram', 'chromagram']\n        for feat_type in feature_types:\n            os.makedirs(os.path.join(output_dir, feat_type), exist_ok=True)\n        \n        # Load audio\n        y = self.load_audio(audio_path)\n        \n        # Calculate STFT (move to GPU if available)\n        stft = torch.stft(\n            y,\n            n_fft=self.n_fft,\n            hop_length=self.hop_length,\n            window=torch.hann_window(self.n_fft).to(self.device),\n            return_complex=True\n        )\n        \n        # Convert to magnitude spectrogram\n        spectrogram = torch.abs(stft).cpu().numpy()\n        \n        # Generate file paths\n        base_filename = os.path.splitext(os.path.basename(audio_path))[0]\n        \n        # 1. Save Spectrogram\n        spec_db = librosa.amplitude_to_db(spectrogram, ref=np.max)\n        spec_path = os.path.join(output_dir, 'spectrogram', f'{base_filename}_spec.png')\n        self.save_feature_image(spec_db, spec_path)\n        \n        # 2. Generate and save Melspectrogram\n        mel_basis = librosa.filters.mel(sr=self.sr, n_fft=self.n_fft, n_mels=self.n_mels)\n        mel_spec = np.dot(mel_basis, spectrogram)\n        mel_spec_db = librosa.amplitude_to_db(mel_spec, ref=np.max)\n        mel_path = os.path.join(output_dir, 'melspectrogram', f'{base_filename}_mel.png')\n        self.save_feature_image(mel_spec_db, mel_path)\n        \n        # 3. Generate and save Chromagram\n        chroma_basis = librosa.filters.chroma(sr=self.sr, n_fft=self.n_fft)\n        chromagram = np.dot(chroma_basis, spectrogram)\n        chroma_path = os.path.join(output_dir, 'chromagram', f'{base_filename}_chroma.png')\n        self.save_feature_image(chromagram, chroma_path, vmin=0)\n        \n        return spec_path, mel_path, chroma_path\n\ndef process_dataset(input_dir, output_dir):\n    \"\"\"Process entire dataset\"\"\"\n    extractor = AudioFeatureExtractor()\n    \n    # Process each COPD level\n    for level in os.listdir(input_dir):\n        level_path = os.path.join(input_dir, level)\n        if not os.path.isdir(level_path):\n            continue\n            \n        # Create output directory for this COPD level\n        level_output = os.path.join(output_dir, level)\n        os.makedirs(level_output, exist_ok=True)\n        \n        # Process all audio files in this level\n        audio_files = [f for f in os.listdir(level_path) if f.endswith('.wav')]\n        for audio_file in tqdm(audio_files, desc=f'Processing {level}'):\n            audio_path = os.path.join(level_path, audio_file)\n            try:\n                extractor.extract_features(audio_path, level_output)\n            except Exception as e:\n                print(f\"Error processing {audio_path}: {str(e)}\")\n\n# Example usage\nif __name__ == \"__main__\":\n    input_dir = \"/kaggle/working/Augmented_COPD_Levels\"\n    output_dir = \"/kaggle/working/Features_Extracted_COPD_Levels\"\n    \n    # Create main output directory\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Process the dataset\n    process_dataset(input_dir, output_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-17T11:53:15.212623Z","iopub.execute_input":"2025-01-17T11:53:15.21296Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\n# Define the folder to compress\nfolder_to_zip = \"/kaggle/working/Augmented_COPD_Levels\"\noutput_zip_file = \"/kaggle/working/Augmented_COPD_Levels.zip\"\n\n# Compress the folder into a zip file\nshutil.make_archive(folder_to_zip, 'zip', folder_to_zip)\n\n# The zip file is now available at /kaggle/working/Augmented_COPD_Levels.zip\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-16T04:47:30.040428Z","iopub.execute_input":"2025-01-16T04:47:30.040754Z","iopub.status.idle":"2025-01-16T04:48:58.067482Z","shell.execute_reply.started":"2025-01-16T04:47:30.040713Z","shell.execute_reply":"2025-01-16T04:48:58.066671Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models, transforms\nfrom PIL import Image\nimport os\nimport numpy as np\nfrom sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom datetime import datetime\nimport json\nfrom tqdm import tqdm\n\nclass COPDDataset(Dataset):\n    def __init__(self, data_dir, transform=None):\n        self.data_dir = data_dir\n        self.transform = transform\n        self.samples = []\n        self.class_counts = {f'COPD{i}': 0 for i in range(5)}\n        \n        # Get all image files and their labels\n        for class_name in os.listdir(data_dir):\n            class_dir = os.path.join(data_dir, class_name)\n            if os.path.isdir(class_dir):\n                label = int(class_name.replace('COPD', ''))\n                self.class_counts[class_name] += len([f for f in os.listdir(class_dir) if f.endswith('.png')])\n                for img_name in os.listdir(class_dir):\n                    if img_name.endswith('.png'):\n                        self.samples.append((os.path.join(class_dir, img_name), label))\n                        \n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        img_path, label = self.samples[idx]\n        image = Image.open(img_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n\nclass ExperimentTracker:\n    def __init__(self, exp_name):\n        self.exp_name = exp_name\n        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        self.results_dir = f'results_{self.exp_name}_{self.timestamp}'\n        os.makedirs(self.results_dir, exist_ok=True)\n        self.metrics = {\n            'train_losses': [], 'train_accs': [],\n            'val_losses': [], 'val_accs': [],\n            'final_test_metrics': None,\n            'confusion_matrix': None,\n            'class_metrics': None,\n            'training_params': None\n        }\n    \n    def save_metrics(self):\n        metrics_file = os.path.join(self.results_dir, 'metrics.json')\n        with open(metrics_file, 'w') as f:\n            json.dump(self.metrics, f, indent=4)\n    \n    def plot_confusion_matrix(self, cm):\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n        plt.title('Confusion Matrix')\n        plt.ylabel('True Label')\n        plt.xlabel('Predicted Label')\n        plt.savefig(os.path.join(self.results_dir, 'confusion_matrix.png'))\n        plt.close()\n\ndef train_model(model, train_loader, val_loader, criterion, optimizer, \n                exp_tracker, num_epochs=50, device='cuda'):\n    print(f\"Using device: {device}\")\n    best_val_acc = 0.0\n    \n    for epoch in tqdm(range(num_epochs), desc=\"Training Progress\"):\n        # Training phase\n        model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            \n            with torch.cuda.amp.autocast():  # Mixed precision training\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n            \n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n        \n        train_loss = running_loss / len(train_loader)\n        train_acc = 100. * correct / total\n        exp_tracker.metrics['train_losses'].append(train_loss)\n        exp_tracker.metrics['train_accs'].append(train_acc)\n        \n        # Validation phase\n        model.eval()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        val_predictions = []\n        val_targets = []\n        \n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                \n                running_loss += loss.item()\n                _, predicted = outputs.max(1)\n                total += labels.size(0)\n                correct += predicted.eq(labels).sum().item()\n                \n                val_predictions.extend(predicted.cpu().numpy())\n                val_targets.extend(labels.cpu().numpy())\n        \n        val_loss = running_loss / len(val_loader)\n        val_acc = 100. * correct / total\n        exp_tracker.metrics['val_losses'].append(val_loss)\n        exp_tracker.metrics['val_accs'].append(val_acc)\n        \n        print(f'Epoch {epoch+1}/{num_epochs}:')\n        print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n        \n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'val_acc': val_acc,\n            }, os.path.join(exp_tracker.results_dir, 'best_resent_model_2.pth'))\n    \n    return model\n\ndef evaluate_model(model, test_loader, criterion, device, exp_tracker):\n    model.eval()\n    all_predictions = []\n    all_targets = []\n    test_loss = 0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for inputs, labels in tqdm(test_loader, desc=\"Evaluating\"):\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            \n            test_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n            \n            all_predictions.extend(predicted.cpu().numpy())\n            all_targets.extend(labels.cpu().numpy())\n    \n    # Calculate metrics\n    test_acc = 100. * correct / total\n    test_loss = test_loss / len(test_loader)\n    cm = confusion_matrix(all_targets, all_predictions)\n    class_metrics = precision_recall_fscore_support(all_targets, all_predictions, average='weighted')\n    \n    # Save results\n    exp_tracker.metrics['final_test_metrics'] = {\n        'test_accuracy': test_acc,\n        'test_loss': test_loss,\n        'precision': class_metrics[0],\n        'recall': class_metrics[1],\n        'f1_score': class_metrics[2]\n    }\n    exp_tracker.metrics['confusion_matrix'] = cm.tolist()\n    exp_tracker.plot_confusion_matrix(cm)\n    \n    # Print detailed classification report\n    print(\"\\nClassification Report:\")\n    print(classification_report(all_targets, all_predictions, \n                              target_names=[f'COPD{i}' for i in range(5)]))\n    \n    return test_acc, test_loss, cm\n\ndef main():\n    # Set random seeds for reproducibility\n    torch.manual_seed(42)\n    torch.cuda.manual_seed(42)\n    np.random.seed(42)\n    \n    # Initialize experiment tracker\n    exp_tracker = ExperimentTracker('COPD_classification')\n    \n    # Set device and enable cudnn benchmarking\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    torch.backends.cudnn.benchmark = True\n    \n    # Define transforms with augmentation for training\n    train_transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomRotation(10),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    val_transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    # Create datasets\n    train_dataset = COPDDataset(\"/kaggle/working/Split_Dataset2/train\", transform=train_transform)\n    val_dataset = COPDDataset(\"/kaggle/working/Split_Dataset2/val\", transform=val_transform)\n    test_dataset = COPDDataset(\"/kaggle/working/Split_Dataset2/test\", transform=val_transform)\n    \n    # Save dataset statistics\n    exp_tracker.metrics['dataset_stats'] = {\n        'train_samples': len(train_dataset),\n        'val_samples': len(val_dataset),\n        'test_samples': len(test_dataset),\n        'class_distribution': train_dataset.class_counts\n    }\n    \n    # Create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, \n                            num_workers=4, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, \n                          num_workers=4, pin_memory=True)\n    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, \n                           num_workers=4, pin_memory=True)\n    \n    # Initialize model\n    model = models.resnet50(pretrained=True)\n    num_ftrs = model.fc.in_features\n    model.fc = nn.Linear(num_ftrs, 5)\n    model = model.to(device)\n    \n    # Define loss function and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n    \n    # Save training parameters\n    exp_tracker.metrics['training_params'] = {\n        'model': 'ResNet50',\n        'optimizer': 'AdamW',\n        'learning_rate': 0.001,\n        'weight_decay': 0.01,\n        'batch_size': 32,\n        'num_epochs': 50\n    }\n    \n    # Train model\n    model = train_model(model, train_loader, val_loader, criterion, optimizer, \n                       exp_tracker, num_epochs=50, device=device)\n    \n    # Evaluate on test set\n    test_acc, test_loss, cm = evaluate_model(model, test_loader, criterion, \n                                           device, exp_tracker)\n    \n    # Save all metrics\n    exp_tracker.save_metrics()\n    \n    print(f\"\\nExperiment complete. Results saved in: {exp_tracker.results_dir}\")\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T15:30:21.952583Z","iopub.execute_input":"2025-01-19T15:30:21.952939Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models, transforms\nfrom PIL import Image\nimport os\n\nclass COPDSpectrogramDataset(Dataset):\n    def __init__(self, data_dir, spec_type='mel', transform=None):\n        \"\"\"\n        Args:\n            data_dir (str): Directory path\n            spec_type (str): One of 'mel', 'spec', or 'chroma'\n            transform: torchvision transforms\n        \"\"\"\n        self.data_dir = data_dir\n        self.transform = transform\n        self.samples = []\n        self.class_counts = {f'COPD{i}': 0 for i in range(5)}\n        \n        # Define file patterns for each spectrogram type\n        spec_patterns = {\n            'mel': 'mel_spectrogram',\n            'spec': 'spectrogram',\n            'chroma': 'chromogram'\n        }\n        self.pattern = spec_patterns[spec_type]\n        \n        # Only load the specified spectrogram type\n        for class_name in os.listdir(data_dir):\n            class_dir = os.path.join(data_dir, class_name)\n            if os.path.isdir(class_dir):\n                label = int(class_name.replace('COPD', ''))\n                for img_name in os.listdir(class_dir):\n                    # Filter files based on spectrogram type\n                    if self.pattern in img_name:\n                        self.samples.append((os.path.join(class_dir, img_name), label))\n                        self.class_counts[class_name] += 1\n    \n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        img_path, label = self.samples[idx]\n        image = Image.open(img_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n\ndef train_separate_models(base_data_dir, num_epochs=50):\n    # Set device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    # Define transforms\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomRotation(10),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    # Train separate models for each spectrogram type\n    for spec_type in ['mel', 'spec', 'chroma']:\n        print(f\"\\nTraining model for {spec_type} spectrograms...\")\n        \n        # Create datasets for current spectrogram type\n        train_dataset = COPDSpectrogramDataset(\n            os.path.join(base_data_dir, 'train'),\n            spec_type=spec_type,\n            transform=transform\n        )\n        val_dataset = COPDSpectrogramDataset(\n            os.path.join(base_data_dir, 'val'),\n            spec_type=spec_type,\n            transform=transform\n        )\n        \n        print(f\"Number of training samples: {len(train_dataset)}\")\n        print(f\"Number of validation samples: {len(val_dataset)}\")\n        print(\"Class distribution:\", train_dataset.class_counts)\n        \n        # Create dataloaders\n        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n        \n        # Initialize model\n        model = models.resnet50(pretrained=True)\n        model.fc = nn.Linear(model.fc.in_features, 5)\n        model = model.to(device)\n        \n        # Training setup\n        criterion = nn.CrossEntropyLoss()\n        optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n        \n        # Train model\n        best_val_acc = 0.0\n        for epoch in range(num_epochs):\n            # Training phase\n            model.train()\n            train_loss = 0.0\n            train_correct = 0\n            train_total = 0\n            \n            for inputs, labels in train_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                optimizer.zero_grad()\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                loss.backward()\n                optimizer.step()\n                \n                _, predicted = outputs.max(1)\n                train_total += labels.size(0)\n                train_correct += predicted.eq(labels).sum().item()\n                train_loss += loss.item()\n            \n            # Validation phase\n            model.eval()\n            val_loss = 0.0\n            val_correct = 0\n            val_total = 0\n            \n            with torch.no_grad():\n                for inputs, labels in val_loader:\n                    inputs, labels = inputs.to(device), labels.to(device)\n                    outputs = model(inputs)\n                    loss = criterion(outputs, labels)\n                    \n                    _, predicted = outputs.max(1)\n                    val_total += labels.size(0)\n                    val_correct += predicted.eq(labels).sum().item()\n                    val_loss += loss.item()\n            \n            # Calculate metrics\n            train_acc = 100. * train_correct / train_total\n            val_acc = 100. * val_correct / val_total\n            \n            print(f'Epoch {epoch+1}/{num_epochs}:')\n            print(f'Train Loss: {train_loss/len(train_loader):.4f}, Train Acc: {train_acc:.2f}%')\n            print(f'Val Loss: {val_loss/len(val_loader):.4f}, Val Acc: {val_acc:.2f}%')\n            \n            # Save best model\n            if val_acc > best_val_acc:\n                best_val_acc = val_acc\n                torch.save({\n                    'epoch': epoch,\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'val_acc': val_acc,\n                }, f'best_model_{spec_type}.pth')\n\ndef main():\n    base_data_dir = \"/kaggle/working/Split_Dataset\"\n    train_separate_models(base_data_dir)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:41:26.279488Z","iopub.execute_input":"2025-01-19T14:41:26.280019Z","iopub.status.idle":"2025-01-19T14:43:47.632203Z","shell.execute_reply.started":"2025-01-19T14:41:26.279988Z","shell.execute_reply":"2025-01-19T14:43:47.630842Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models, transforms\nfrom PIL import Image\nimport os\nimport numpy as np\nfrom sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport json\nfrom tqdm import tqdm\nfrom datetime import datetime\n\n\nclass COPDDataset(Dataset):\n    def __init__(self, data_dir, transform=None):\n        self.data_dir = data_dir\n        self.transform = transform\n        self.samples = []\n        self.class_counts = {f'COPD{i}': 0 for i in range(5)}\n        \n        # Get all image files with '_chromogram' in their names and their labels\n        for class_name in os.listdir(data_dir):\n            class_dir = os.path.join(data_dir, class_name)\n            if os.path.isdir(class_dir):\n                label = int(class_name.replace('COPD', ''))\n                for img_name in os.listdir(class_dir):\n                    if '_chromogram' in img_name and img_name.endswith('.png'):\n                        self.samples.append((os.path.join(class_dir, img_name), label))\n                        self.class_counts[class_name] += 1\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        img_path, label = self.samples[idx]\n        image = Image.open(img_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n\n\nclass ExperimentTracker:\n    def __init__(self, exp_name):\n        self.exp_name = exp_name\n        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        self.results_dir = f'results_{self.exp_name}_{self.timestamp}'\n        os.makedirs(self.results_dir, exist_ok=True)\n        self.metrics = {\n            'train_losses': [], 'train_accs': [],\n            'val_losses': [], 'val_accs': [],\n            'final_test_metrics': None,\n            'confusion_matrix': None,\n            'class_metrics': None,\n            'training_params': None\n        }\n    \n    def save_metrics(self):\n        metrics_file = os.path.join(self.results_dir, 'metrics.json')\n        with open(metrics_file, 'w') as f:\n            json.dump(self.metrics, f, indent=4)\n    \n    def plot_confusion_matrix(self, cm):\n        plt.figure(figsize=(10, 8))\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n        plt.title('Confusion Matrix')\n        plt.ylabel('True Label')\n        plt.xlabel('Predicted Label')\n        plt.savefig(os.path.join(self.results_dir, 'confusion_matrix.png'))\n        plt.close()\n\n\ndef train_model(model, train_loader, val_loader, criterion, optimizer, \n                exp_tracker, num_epochs=50, device='cuda'):\n    print(f\"Using device: {device}\")\n    best_val_acc = 0.0\n    \n    for epoch in tqdm(range(num_epochs), desc=\"Training Progress\"):\n        model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            \n            with torch.cuda.amp.autocast():  # Mixed precision training\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n            \n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n        \n        train_loss = running_loss / len(train_loader)\n        train_acc = 100. * correct / total\n        exp_tracker.metrics['train_losses'].append(train_loss)\n        exp_tracker.metrics['train_accs'].append(train_acc)\n        \n        model.eval()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        val_predictions = []\n        val_targets = []\n        \n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                \n                running_loss += loss.item()\n                _, predicted = outputs.max(1)\n                total += labels.size(0)\n                correct += predicted.eq(labels).sum().item()\n                \n                val_predictions.extend(predicted.cpu().numpy())\n                val_targets.extend(labels.cpu().numpy())\n        \n        val_loss = running_loss / len(val_loader)\n        val_acc = 100. * correct / total\n        exp_tracker.metrics['val_losses'].append(val_loss)\n        exp_tracker.metrics['val_accs'].append(val_acc)\n        \n        print(f'Epoch {epoch+1}/{num_epochs}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n        print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n        \n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'val_acc': val_acc,\n            }, os.path.join(exp_tracker.results_dir, 'best_resnet_model.pth'))\n    \n    return model\n\n\ndef evaluate_model(model, test_loader, criterion, device, exp_tracker):\n    model.eval()\n    all_predictions = []\n    all_targets = []\n    test_loss = 0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for inputs, labels in tqdm(test_loader, desc=\"Evaluating\"):\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            \n            test_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n            \n            all_predictions.extend(predicted.cpu().numpy())\n            all_targets.extend(labels.cpu().numpy())\n    \n    test_acc = 100. * correct / total\n    test_loss = test_loss / len(test_loader)\n    cm = confusion_matrix(all_targets, all_predictions)\n    class_metrics = precision_recall_fscore_support(all_targets, all_predictions, average='weighted')\n    \n    exp_tracker.metrics['final_test_metrics'] = {\n        'test_accuracy': test_acc,\n        'test_loss': test_loss,\n        'precision': class_metrics[0],\n        'recall': class_metrics[1],\n        'f1_score': class_metrics[2]\n    }\n    exp_tracker.metrics['confusion_matrix'] = cm.tolist()\n    exp_tracker.plot_confusion_matrix(cm)\n    \n    print(\"\\nClassification Report:\")\n    print(classification_report(all_targets, all_predictions, \n                              target_names=[f'COPD{i}' for i in range(5)]))\n    return test_acc, test_loss, cm\n\n\ndef main():\n    torch.manual_seed(42)\n    torch.cuda.manual_seed(42)\n    np.random.seed(42)\n    \n    exp_tracker = ExperimentTracker('COPD_classification')\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    torch.backends.cudnn.benchmark = True\n    \n    train_transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomRotation(10),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    val_transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    train_dataset = COPDDataset(\"/kaggle/working/Split_Dataset/train\", transform=train_transform)\n    val_dataset = COPDDataset(\"/kaggle/working/Split_Dataset/val\", transform=val_transform)\n    test_dataset = COPDDataset(\"/kaggle/working/Split_Dataset/test\", transform=val_transform)\n    \n    exp_tracker.metrics['dataset_stats'] = {\n        'train_samples': len(train_dataset),\n        'val_samples': len(val_dataset),\n        'test_samples': len(test_dataset),\n        'class_distribution': train_dataset.class_counts\n    }\n    \n    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, \n                              num_workers=4, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, \n                            num_workers=4, pin_memory=True)\n    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, \n                             num_workers=4, pin_memory=True)\n    \n    model = models.resnet50(pretrained=True)\n    num_ftrs = model.fc.in_features\n    model.fc = nn.Linear(num_ftrs, 5)\n    model = model.to(device)\n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n    \n    exp_tracker.metrics['training_params'] = {\n        'model': 'ResNet50',\n        'optimizer': 'AdamW',\n        'learning_rate': 0.001,\n        'weight_decay': 0.01,\n        'batch_size': 32,\n        'num_epochs': 50\n    }\n    \n    model = train_model(model, train_loader, val_loader, criterion, optimizer, \n                        exp_tracker, num_epochs=50, device=device)\n    \n    test_acc, test_loss, cm = evaluate_model(model, test_loader, criterion, \n                                             device, exp_tracker)\n    \n    exp_tracker.save_metrics()\n    print(f\"\\nExperiment complete. Results saved in: {exp_tracker.results_dir}\")\n\n\nif __name__ == '__main__':\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T15:11:05.620269Z","iopub.execute_input":"2025-01-19T15:11:05.620612Z","iopub.status.idle":"2025-01-19T15:17:30.563834Z","shell.execute_reply.started":"2025-01-19T15:11:05.62058Z","shell.execute_reply":"2025-01-19T15:17:30.562632Z"}},"outputs":[],"execution_count":null}]}